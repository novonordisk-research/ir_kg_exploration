{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a02f0d1-4326-4d53-b54c-bec55936ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pulearn.elkanoto import ElkanotoPuClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ParameterGrid, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pul_config\n",
    "import importlib\n",
    "importlib.reload(pul_config)\n",
    "\n",
    "def predict_proba(m, X):\n",
    "    try:\n",
    "        proba = m.predict_proba(X)\n",
    "    except:\n",
    "        try:\n",
    "            proba = m.decision_function(X)\n",
    "        except:\n",
    "            proba = m.best_estimator_.decision_function(X)\n",
    "\n",
    "    return proba\n",
    "\n",
    "def evaluate(m, X, y, n=100, scale=True):\n",
    "    y_pred = m.predict(X) > 0\n",
    "    y_prob_pred = predict_proba(m, X)\n",
    "\n",
    "    order = np.argsort(y_prob_pred)[::-1]\n",
    "    # print(y_pred[order][:n])\n",
    "    if scale:\n",
    "        return y[order][:n].sum() / y.sum()\n",
    "    else:\n",
    "        return y[order][:n].sum()\n",
    "    \n",
    "\n",
    "# CHANGE HERE\n",
    "input_dirs = [Path('1_outputs/standard/'), Path('1_outputs/small_DS/')]\n",
    "output_dirs = [Path('2_outputs/standard/'), Path('2_outputs/small_DS/')]\n",
    "method = 'IsolationForest'\n",
    "pul_cfg = pul_config.IsolationForestConfig\n",
    "# -----\n",
    "\n",
    "MODEL_NAMES = ('RotatE', 'TransE')\n",
    "output_dirs = [output_dir / method for output_dir in output_dirs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceddba04-3d10-45f5-867d-7f1fe35cdb47",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9385edac-59fb-4c24-8fe5-d2d32c65e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff31689c-1a2e-414d-9572-b333d6793ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_dir, output_dir, n_jobs, cv, model_names, rerun=False):\n",
    "    ref_df = pd.read_csv(input_dir / 'ref_df.csv', index_col=0)\n",
    "    ref_df.head()\n",
    "    for model_name in tqdm(model_names):\n",
    "        out_dir = output_dir / model_name\n",
    "        out_dir.mkdir(exist_ok=True, parents=True)\n",
    "        for i in tqdm(range(10)):\n",
    "            m_file = out_dir / f'{model_name}_{i}.pkl'\n",
    "            if m_file.exists():\n",
    "                try:\n",
    "                    # try to load the file\n",
    "                    with open(m_file, 'rb') as f:\n",
    "                        pickle.load(f)\n",
    "                    if not rerun:\n",
    "                        print(m_file, 'exists. Skipping...')\n",
    "                        continue\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            X_all = np.load(input_dir / f'{model_name}_X_{i}.npy')\n",
    "\n",
    "            ds_names = ('train', 'test', 'val')\n",
    "            Xs = {ds: X_all[ref_df.query(ds)['id']] for ds in ds_names}\n",
    "            ys = {ds: ref_df.query(ds)['y'].values for ds in ds_names}\n",
    "\n",
    "            ds = 'train'\n",
    "            X = Xs[ds]\n",
    "            y = ys[ds]\n",
    "\n",
    "            pipe = pul_cfg.build_pipeline()\n",
    "            param_grid = pul_cfg.build_param_grid()\n",
    "\n",
    "            grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=cv, refit=True, n_jobs=n_jobs, scoring=evaluate)\n",
    "            grid_search.fit(X=X, y=y)\n",
    "\n",
    "            with open(m_file, 'wb') as f:\n",
    "                pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002009b4-f5f6-4db7-8059-35bbe07d3432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 83.68it/s][A\u001b[A\n",
      "\n",
      " 50%|█████     | 1/2 [00:00<00:00,  7.92it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_outputs/standard/IsolationForest/RotatE/RotatE_0.pkl exists. Skipping...\n",
      "2_outputs/standard/IsolationForest/RotatE/RotatE_1.pkl exists. Skipping...\n",
      "2_outputs/standard/IsolationForest/RotatE/RotatE_2.pkl exists. Skipping...\n",
      "2_outputs/standard/IsolationForest/RotatE/RotatE_3.pkl exists. Skipping...\n",
      "2_outputs/standard/IsolationForest/RotatE/RotatE_4.pkl exists. Skipping...\n",
      "2_outputs/standard/IsolationForest/RotatE/RotatE_5.pkl exists. Skipping...\n",
      "2_outputs/standard/IsolationForest/RotatE/RotatE_6.pkl exists. Skipping...\n",
      "2_outputs/standard/IsolationForest/RotatE/RotatE_7.pkl exists. Skipping...\n",
      "2_outputs/standard/IsolationForest/RotatE/RotatE_8.pkl exists. Skipping...\n",
      "2_outputs/standard/IsolationForest/RotatE/RotatE_9.pkl exists. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_outputs/standard/IsolationForest/TransE/TransE_0.pkl exists. Skipping...\n",
      "2_outputs/standard/IsolationForest/TransE/TransE_1.pkl exists. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 30%|███       | 3/10 [00:00<00:00, 27.88it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_outputs/standard/IsolationForest/TransE/TransE_2.pkl exists. Skipping...\n",
      "2_outputs/standard/IsolationForest/TransE/TransE_3.pkl exists. Skipping...\n",
      "2_outputs/standard/IsolationForest/TransE/TransE_4.pkl exists. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 60%|██████    | 6/10 [00:00<00:00, 28.06it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_outputs/standard/IsolationForest/TransE/TransE_5.pkl exists. Skipping...\n",
      "2_outputs/standard/IsolationForest/TransE/TransE_6.pkl exists. Skipping...\n",
      "2_outputs/standard/IsolationForest/TransE/TransE_7.pkl exists. Skipping...\n",
      "2_outputs/standard/IsolationForest/TransE/TransE_8.pkl exists. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 30.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.35it/s]\u001b[A\n",
      " 50%|█████     | 1/2 [00:00<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_outputs/standard/IsolationForest/TransE/TransE_9.pkl exists. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 1/10 [01:44<15:44, 104.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 2/10 [03:12<12:36, 94.62s/it] \u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 3/10 [04:39<10:38, 91.26s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 4/10 [06:05<08:54, 89.04s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 5/10 [07:29<07:16, 87.37s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 6/10 [08:48<05:37, 84.43s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 7/10 [10:07<04:08, 82.85s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 8/10 [11:29<02:44, 82.32s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 9/10 [12:48<01:21, 81.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 10/10 [14:07<00:00, 84.74s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 1/2 [14:07<14:07, 847.40s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "N_JOBS = 40\n",
    "CV = 5\n",
    "\n",
    "for input_dir, output_dir in tqdm(zip(input_dirs, output_dirs), total=len(input_dirs)):\n",
    "    train(input_dir=input_dir, output_dir=output_dir, model_names=MODEL_NAMES, cv=CV, n_jobs=N_JOBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e41f459-c208-4cfb-9ea6-bd7547aa6343",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a4492a-317a-4709-abcd-44715f837988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grid_searches(output_dir, model_names):\n",
    "    grid_searches = {}\n",
    "    for model_name in model_names:\n",
    "        grid_searches[model_name] = {}\n",
    "        for j in range(10):\n",
    "            m_name = f'{model_name}_{j}'\n",
    "            m_file = output_dir / model_name / f'{m_name}.pkl'\n",
    "            with open(m_file, 'rb') as f:\n",
    "                grid_search = pickle.load(f)\n",
    "            grid_searches[model_name][m_name] = grid_search\n",
    "\n",
    "    return grid_searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89951ebd-8ffa-4f38-800e-0681a7558fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_searches_list = [\n",
    "    load_grid_searches(output_dir=o, model_names=MODEL_NAMES) for o in output_dirs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1411d61-1f00-411f-8b6a-2e19e3d4a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_df_from_grid_searches(grid_searches, input_dir, ns=(10,100)):\n",
    "    ref_df = pd.read_csv(input_dir / 'ref_df.csv', index_col=0)\n",
    "    ref_df.head()\n",
    "\n",
    "    score_dict = {\n",
    "        'model_name': [],\n",
    "        'm_name': [],\n",
    "        'score10_val': [],\n",
    "        'score100_val': [],\n",
    "        'score10_test': [],\n",
    "        'score100_test': [],\n",
    "\n",
    "    }\n",
    "    scale = False\n",
    "    for model_name in grid_searches.keys():\n",
    "        for i, m_name in enumerate(grid_searches[model_name].keys()):\n",
    "            score_dict['model_name'].append(model_name)\n",
    "            score_dict['m_name'].append(m_name)\n",
    "            X_all = np.load(input_dir / f'{model_name}_X_{i}.npy')\n",
    "\n",
    "            ds_names = ('train', 'test', 'val')\n",
    "            Xs = {ds: X_all[ref_df.query(ds)['id']] for ds in ds_names}\n",
    "            ys = {ds: ref_df.query(ds)['y'].values for ds in ds_names}\n",
    "\n",
    "            grid_search = grid_searches[model_name][m_name]\n",
    "            for n in ns:\n",
    "                val_score = evaluate(\n",
    "                    m=grid_search,\n",
    "                    X=Xs['val'],\n",
    "                    y=ys['val'],\n",
    "                    n=n,\n",
    "                    scale=scale\n",
    "                )\n",
    "                score_dict[f'score{n}_val'].append(val_score)\n",
    "\n",
    "                test_score = evaluate(\n",
    "                    m=grid_search,\n",
    "                    X=Xs['test'],\n",
    "                    y=ys['test'],\n",
    "                    n=n,\n",
    "                    scale=scale\n",
    "                )\n",
    "                score_dict[f'score{n}_test'].append(test_score)\n",
    "\n",
    "    score_df = pd.DataFrame(score_dict)\n",
    "    return score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a78ec72-80f8-4a6e-b4ed-0474171ae40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dfs = [score_df_from_grid_searches(gs, i) for gs, i in zip(grid_searches_list, input_dirs)]\n",
    "for output_dir, score_df in zip(output_dirs, score_dfs):\n",
    "    score_df.to_csv(output_dir / 'score_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc17ed-9cc9-4019-88ed-4c1d3df5e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dfs[0].groupby('model_name').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d93785-b64c-4ee4-bbf2-ee1ad76f3eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(score_dfs[0], x='model_name', y='score10_test').set(title='Hits@100 and hits@10\\n(large positive set)')\n",
    "sns.boxplot(score_dfs[0], x='model_name', y='score100_test').set_ylabel('score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f12e20-1513-4bb6-af4e-bb1f03aa33c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dfs[1].groupby('model_name').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ba3e17-9ce9-4e0e-9a86-40fccc97591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(score_dfs[1], x='model_name', y='score10_test').set(title='Hits@100 and hits@10\\n(small positive set)')\n",
    "sns.boxplot(score_dfs[1], x='model_name', y='score100_test').set_ylabel('score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda037b9-a72c-4f6f-bc2b-fec19c4dfea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dfs = []\n",
    "for output_dir in output_dirs:\n",
    "    score_dfs.append(pd.read_csv(output_dir / 'score_df.csv', index_col=0))\n",
    "plot_dfs = []\n",
    "for ds, score_df in zip(('large', 'small'), score_dfs):\n",
    "    plot_df = score_df[['model_name', 'score10_test', 'score100_test']].copy()\n",
    "    plot_df.columns = ['model_name', 'hits@10', 'hits@100']\n",
    "    plot_df['dataset'] = ds\n",
    "    plot_dfs.append(plot_df)\n",
    "    \n",
    "plot_df = pd.concat(plot_dfs)\n",
    "plot_df = plot_df.melt(id_vars=['model_name', 'dataset'])\n",
    "plot_dfs = {k:v for k, v in plot_df.groupby('model_name')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300de283-ff58-4dd2-9e47-41270a846bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "model_name = 'TransE'\n",
    "plot_df = plot_dfs[model_name]\n",
    "plot = sns.barplot(plot_df, x='dataset', y='value', hue='variable', order=['small', 'large'])\n",
    "sns.move_legend(plot, 'upper left')\n",
    "plot.set(xlabel='positive dataset', ylabel='score')\n",
    "plot.set_title(f'Isolation Forest - {model_name}')\n",
    "plot.set_ylim([0, 55])\n",
    "_ = plot.set_xticklabels(labels=['small', 'large'], rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae61d1f6-7a6c-4e79-be96-9c428d48faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "model_name = 'RotatE'\n",
    "plot_df = plot_dfs[model_name]\n",
    "plot = sns.barplot(plot_df, x='dataset', y='value', hue='variable', order=['small', 'large'])\n",
    "sns.move_legend(plot, 'upper left')\n",
    "plot.set(xlabel='positive dataset', ylabel='score')\n",
    "plot.set_title(f'Isolation Forest - {model_name}')\n",
    "plot.set_ylim([0, 55])\n",
    "_ = plot.set_xticklabels(labels=['small', 'large'], rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ac18e-d767-4740-a6d2-f42e0e6d82a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_predictions(grid_searches, input_dir):\n",
    "    ref_df = pd.read_csv(input_dir / 'ref_df.csv', index_col=0)\n",
    "    ref_df.head()\n",
    "    \n",
    "    ids = ref_df['id'].values\n",
    "    y = ref_df['y'].values\n",
    "    train = ref_df['train'].values\n",
    "    val = ref_df['val'].values\n",
    "    test = ref_df['test'].values\n",
    "    \n",
    "    y_preds = {}\n",
    "    for model_name in grid_searches.keys():\n",
    "        y_preds[model_name] = {}\n",
    "        for i, m_name in enumerate(grid_searches[model_name].keys()):\n",
    "            X_all = np.load(input_dir / f'{model_name}_X_{i}.npy')\n",
    "            X = X_all[ids]\n",
    "\n",
    "            grid_search = grid_searches[model_name][m_name]\n",
    "            y_preds[model_name][m_name] = predict_proba(grid_search, X)\n",
    "            \n",
    "    return y_preds\n",
    "\n",
    "def build_proba_df(grid_searches, input_dir):\n",
    "    from pykeen.datasets import OpenBioLink\n",
    "    obl = OpenBioLink()\n",
    "    id_to_entity = {i:e for e, i in obl.entity_to_id.items()}\n",
    "    \n",
    "    y_preds = calc_predictions(grid_searches=grid_searches, input_dir=input_dir)\n",
    "    \n",
    "    ref_df = pd.read_csv(input_dir / 'ref_df.csv', index_col=0)\n",
    "    ref_df.head()\n",
    "    \n",
    "    ids = ref_df['id'].values\n",
    "    y = ref_df['y'].values\n",
    "    train = ref_df['train'].values\n",
    "    val = ref_df['val'].values\n",
    "    test = ref_df['test'].values\n",
    "    \n",
    "    proba_df = pd.DataFrame(dict(\n",
    "        id=ids,\n",
    "        entity=[id_to_entity[i] for i in ids],\n",
    "        y=y,\n",
    "        train=train,\n",
    "        val=val,\n",
    "        test=test,\n",
    "        **y_preds['RotatE'],\n",
    "        **y_preds['TransE'],\n",
    "    ))\n",
    "    proba_df['RotatE_sum'] = proba_df.filter(regex='RotatE_[0-9]+').sum(axis=1)\n",
    "    proba_df['TransE_sum'] = proba_df.filter(regex='TransE_[0-9]+').sum(axis=1)\n",
    "    \n",
    "    return proba_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae05b4d-b629-4674-b623-dda434899515",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_dfs = [build_proba_df(gs, i) for gs, i in zip(grid_searches_list, input_dirs)]\n",
    "for output_dir, proba_df in zip(output_dirs, proba_dfs):\n",
    "    proba_df.to_csv(output_dir / 'proba_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c84bc-4a73-4ed7-b818-15f99a1d57d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_df = proba_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a0d487-2430-4cc9-9667-c0e7d708bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_df.query('test').sort_values('RotatE_sum', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "giannt",
   "language": "python",
   "name": "giannt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
